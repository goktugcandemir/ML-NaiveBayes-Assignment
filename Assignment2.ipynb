{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part I: Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MLE</h3>\n",
    "<img src=\"images/t1.jpeg\"><br>\n",
    "<img src=\"images/t2.jpeg\"><br>\n",
    "<img src=\"images/t3.jpeg\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tablo.png\"><br>\n",
    "Q1)For a car with features we want to classify is it stolen or not? Attributes are Color,\n",
    "Type, Origin and subject is stolen, that can be yes or no.\n",
    "Is a Red Domestic SUV is stolen or not (Note there is no example of a Red Domestic\n",
    "SUV in our dataset)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b> \n",
    "P(Stolen=Yes)=0.5   P(Stolen=No)=0.5\n",
    "\n",
    "P(Stolen=Yes | X ) => P(Red|Stolen=Yes) * P(SUV|Stolen=Yes) | P(Domestic|Stolen=Yes)\n",
    "P(Red|Stolen=Yes)=3/5\n",
    "P(SUV|Stolen=Yes)=2/5\n",
    "P(Domestic|Stolen=Yes)=3/5\n",
    "So Probability of Stolen Red SUV Domestic car is P(Stolen=Yes) * P(Stolen=Yes | X ) = 0.5 * 0.6 * 0.4 * 0.6 =0,072\n",
    "<br>\n",
    "\n",
    "P(Stolen=No | X ) => P(Red|Stolen=No) * P(SUV|Stolen=No) | P(Domestic|Stolen=No)\n",
    "P(Red|Stolen=Yes)=2/5\n",
    "P(SUV|Stolen=Yes)=2/5\n",
    "P(Domestic|Stolen=Yes)=2/5\n",
    "So Probability of Not Stolen Red SUV Domestic car is P(Stolen=No) * P(Stolen=No | X ) = 0.5 * 0.4 * 0.4 * 0.4 =0,032\n",
    "\n",
    "Probability of Stolen Red SUV Domestic > Probability of Not Stolen Red SUV Domestic \n",
    "So our decision is  Red SUV Domestic car is stolen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2)<img src=\"images/tablo3.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Using Naive Bayes, what is the probability that a person who is 'not rich',\n",
    "'married' and 'healthy' is 'content'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b> P(Rich=No | Content=Yes)=1/4  P(Married=Yes | Content=Yes)=2/4  P(Healthy=Yes | Content=Yes)=3/4   P(Content=Yes)=4/9\n",
    "So Probability that a person who is not rich , married and healthly is = 1/4 * 2/4 * 3/4 *4/9 = 0.041\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-What is the probability that a person who is 'not rich' and 'married' is 'content'?\n",
    "(That is, we do not know whether ot not they are 'healthy'.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>   P(Rich=No | Content=Yes)=1/4  P(Married=Yes | Content=Yes)=2/4 P(Content=Yes)=4/9 \n",
    "<p>So probability of 'not rich' and 'married' is 'content' = 1/4 * 2/4 * 4/9 = 0.055</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PART II: Sentiment Analysis with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we tried to guess whether the comments in the given data were a positive or a negative one, simply using Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1) Understanding the data </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 11.914 comments.Before looking at the dataset, I expect the words <b>like, love, great, good</b> which we use the most when making positive comments in daily life, to appear frequently in sentences, and I also anticipate that the words <b>not, bad, terrible,would</b> will appear more often in negative sentences.\n",
    "<p>It is necessary to wait 4-5 minutes for the data reading and preprocessing phase. At the end of these processes, the naivebayes algorithm ends in 10-15 seconds.<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2) Implementing Naive Bayes </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For the Naive Bayes Algorithm, we need to extract all the different words in the comments. To do this, I used CountVectorizer from the scikit-learn library and created the Bag of Words (BoW) dictionary. I also created separate BoWs for positive sentences and separate negative sentences. Thus, we can distinguish and interpret the words in positive and negative sentences more easily.In summary, I kept 3 BoW in total. BoW for positive reviews, Bow for negative reviews and BoW for all reviews.I allocated 80% of the given data for the train and 20% for the test. Thanks to Naive Bayes, I could successfully estimate sentiment and category of sentence .End of the Naive Bayes algorithm I got a successful result from unseen test data.One of the results I got is as follows.</p><br><img src=\"images/nb.png\"><br>\n",
    "<p><b>BoW Unigram-Bigram Decision</b></p>\n",
    "I had two options for BoW. Unigram and Bigram.Bigram gave better accuracy than Unigram, but since it takes all binary words, the number of features is considerably higher than unigram and increases computational time much more. That's why I chose to use Unigram.\n",
    "<img src=\"images/bow.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is evident in the histogram chart.\n",
    "<li>UPositiveBow : Unigram Positive Bow </li>\n",
    "<li>UNegativeBow : Unigram Negative Bow</li>\n",
    "<li>BPositiveBow : Bigram Positive Bow</li>\n",
    "<li>BNegativeBow : Bigram Negative Bow</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the log probabilities to prevent numerical underflow. Accuracy increased from 85% to 90% thanks to this operation.\n",
    "<br><img src=\"images/log.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied Laplace smoothing so that the naive bayes result does not come out 0 when a word that has never been encountered before in the test phase. When such a word comes, the count value becomes 0, and we take count + 1 when calculating, so we prevented the naive bayes result from coming to 0.The same operation has done for negative BoW<br>\n",
    "<img src=\"images/laplace.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3) Error Analysis</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one of the misclassified review samples. <br>This is a sample of positive reviews from 857.txt and the topic is software. Our prediction was negative for this sentence but it was positive.\n",
    "\n",
    "<li>\"never understand others would want use anything else tried four others one best using product starting upgrade every four year\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sentence is difficult to understand and predict, because nothing positive or negative has been said clearly. We guess this sentence incorrectly because it contains words such as <b>would, use, one</b> that appear a lot both in negative sentences and possitive sentence and they are not dominant enough to discern. The picture showing the most common words in positive and negative sentences is shown in the module analysis section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>4) Modul Analysis </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I processed the sentences before adding them to the Naive Bayes algorithm. I deleted the punctuation marks in the sentence and deleted the stopwords in the sentence. Because stopwords and punctuation marks are abundant in both positive and negative sentences. It has the effect of reducing the influence of other words, which are important in decision making, on the prediction. This affects the prediction negatively.TD-IDF can also be used for the same purpose. I've also experienced that it has a positive effect when I use TD-IDF.\n",
    "\n",
    "<p>To measure the effect of words on predictions, we created the dictionary of negativeBow and positiveBoW. Key represents the word and value is the number of occurrences in all sentences.<p>\n",
    "<p><b>20 words whose presence most strongly predicts that the review positive<b><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/positive.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>20 words whose presence most strongly predicts that the review negative<b><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/negative.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the images, words such as <b>also, album, great, good, well, love, best </b> are found much more than the negative list when predicting and play an important role in the prediction of being positive. In addition, the words <b>would get, even, could, product, better, work and film </b> are more common in negative lists than positive, and they have an intense effect on negatively predicting sentences containing these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, it is seen that words such as <b>one, like,camera, really </b> are one of the most used words in both dictionaries. These words do not have a decisive effect on the result when guessing because they are likely to be found on both sides and are close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>5) Calculation of Accuracy</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a confusion matrix to calculate accuracy. Thanks to the confusion matrix, I divided the number of correctly predicted predictions by the total number of predictions and multiplied by 100, so I determined the accuracy value in %.Here is my confusion matrix and accuracy calculation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/acc.png\"><br><img src=\"images/cm.png\"><br>When I tested using the confusion matrix, I got a successful result. Aaccuracy value was found to be approximately between 90% and 91%.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6) Bonus Implement a six-category classifier</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the same method for category prediction that I did for sentiment prediction.I created a BoW for each category and calculated the probability of which category it belongs to for each sentence. I transferred the result in a confusion matrix and calculated accuracy. I found a highly successful accuracy value as a result of the calculation. I made 90.93% correct guesses.The resulting confusion matrix is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cm2.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Console output is also like this.\n",
    "<img src=\"images/console.png\"><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
